{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddc8d01",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c741620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv('virusshare.csv', sep=',',skiprows=1, header=None).to_numpy()\n",
    "\n",
    "X = data[:,1:]\n",
    "Y = data[:,0]\n",
    "\n",
    "def testModel(model,modelName):\n",
    "    kf = KFold(n_splits=5)\n",
    "    acc_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        train_X, test_X = X[train_index], X[test_index]\n",
    "        train_Y, test_Y = Y[train_index], Y[test_index]\n",
    "        model.fit(train_X,train_Y)\n",
    "        pred_values = model.predict(test_X)\n",
    "        acc = accuracy_score(pred_values , test_Y)\n",
    "        f1 = f1_score(pred_values , test_Y)\n",
    "        acc_scores.append(acc)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    avg_acc_score = sum(acc_scores)/5\n",
    "    avg_f1_score = sum(f1_scores)/5\n",
    "\n",
    "    print('Method: '+modelName)\n",
    "    print('accuracy of each fold - {}'.format(acc_scores))\n",
    "    print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "    print('f1 of each fold - {}'.format(f1_scores))\n",
    "    print('Avg f1 : {}'.format(avg_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab29983",
   "metadata": {},
   "source": [
    "# Logistic Regression (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72f25340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Ridge logistic regression\n",
      "accuracy of each fold - [0.67475, 0.695, 0.68475, 0.68, 0.685]\n",
      "Avg accuracy : 0.6839000000000001\n",
      "f1 of each fold - [0.6882338844955668, 0.7174617878647522, 0.6873295313662288, 0.6842624568327578, 0.6987087517934002]\n",
      "Avg f1 : 0.6951992824705412\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "testModel(LogisticRegression(solver = \"liblinear\", penalty = \"l2\"),\"Ridge logistic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62306d9",
   "metadata": {},
   "source": [
    "# Logistic Regression (LASSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5c3ee29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: LASSO logistic regression\n",
      "accuracy of each fold - [0.6755, 0.69525, 0.6835, 0.6805, 0.684]\n",
      "Avg accuracy : 0.6837500000000001\n",
      "f1 of each fold - [0.6891762452107278, 0.717497103128621, 0.6872529644268774, 0.685066535239034, 0.6971729755630092]\n",
      "Avg f1 : 0.6952331647136539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "testModel(LogisticRegression(solver = \"liblinear\", penalty = \"l1\"),\"LASSO logistic regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ce8b5",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74fd31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: multilayer perceptron\n",
      "accuracy of each fold - [0.79775, 0.7895, 0.79875, 0.80075, 0.80125]\n",
      "Avg accuracy : 0.7976\n",
      "f1 of each fold - [0.7956554685526648, 0.7900249376558603, 0.7943805874840357, 0.8023803620133894, 0.8010012515644556]\n",
      "Avg f1 : 0.7966885214540811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "testModel(MLPClassifier(solver='sgd', learning_rate = \"adaptive\", learning_rate_init = 0.1, alpha=1e-5,hidden_layer_sizes=(10, 2), random_state=1,max_iter=500),\"multilayer perceptron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c65aac",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9c700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "testModel(LinearSVC(penalty = \"l1\", loss=\"squared_hinge\",dual=False, max_iter = 10000, tol = 0.001),\"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ad073",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "testModel(DecisionTreeClassifier(random_state=0),\"decision tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de306e0d",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffccc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "testModel(RandomForestClassifier(max_depth=300, random_state=0),\"random forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8616a5",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993dbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "testModel(GaussianNB(),\"Gaussian Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68e0b15",
   "metadata": {},
   "source": [
    "# k Nearest Neighbors (cosine distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "testModel(KNeighborsClassifier(n_neighbors=5, metric=\"cosine\"),\"kNN (cosine)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70008d70",
   "metadata": {},
   "source": [
    "# k Nearest Neighbors (Euclidean distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c7622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "testModel(KNeighborsClassifier(n_neighbors=5, metric=\"euclidean\"),\"kNN (Euclidean)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6986f9",
   "metadata": {},
   "source": [
    "# Ridge Logistic Regression + AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c54aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "ridge = LogisticRegression(solver = \"liblinear\",penalty = \"l2\")\n",
    "testModel(AdaBoostClassifier(estimator = ridge, n_estimators=30),\"ridge + AdaBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca000901",
   "metadata": {},
   "source": [
    "# Random Forest + AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7eeee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "testModel(AdaBoostClassifier(estimator = rf, n_estimators=10),\"random forest + AdaBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c2c0d",
   "metadata": {},
   "source": [
    "# FTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datatable import Frame\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\"\"\"\n",
    "\n",
    "data = pd.read_csv('virusshare.csv', sep=',',skiprows=1, header=None).to_numpy()\n",
    "\n",
    "X = data[:,1:]\n",
    "Y = data[:,0]\n",
    "\n",
    "def testFTRL(model,modelName):\n",
    "    kf = KFold(n_splits=5)\n",
    "    acc_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        train_X, test_X = Frame(X[train_index]), Frame(X[test_index])\n",
    "        train_Y, test_Y = Frame(Y[train_index]), Frame(Y[test_index])\n",
    "        model.fit(train_X,train_Y)\n",
    "        pred_values = model.predict(test_X).to_numpy()\n",
    "        pred_values = np.rint(pred_values)\n",
    "        acc = accuracy_score(pred_values, Y[test_index])\n",
    "        f1 = f1_score(pred_values , Y[test_index])\n",
    "        acc_scores.append(acc)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    avg_acc_score = sum(acc_scores)/5\n",
    "    avg_f1_score = sum(f1_scores)/5\n",
    "\n",
    "    print('Method: '+modelName)\n",
    "    print('accuracy of each fold - {}'.format(acc_scores))\n",
    "    print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "    print('f1 of each fold - {}'.format(f1_scores))\n",
    "    print('Avg f1 : {}'.format(avg_f1_score))\n",
    "from datatable.models import Ftrl\n",
    "testFTRL(Ftrl(),\"FTRL\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3c9ab",
   "metadata": {},
   "source": [
    "Since Jupyter Notebook fails to detect the datatable package, the results are listed in text form.\n",
    "\n",
    "Method: FTRL\n",
    "\n",
    "accuracy of each fold - [0.8065, 0.80125, 0.7935, 0.796, 0.80175]\n",
    "\n",
    "Avg accuracy : 0.7998000000000001\n",
    "\n",
    "f1 of each fold - [0.7913746630727764, 0.785540868626922, 0.7750544662309369, 0.7807630306286942, 0.8050159822965332]\n",
    "\n",
    "Avg f1 : 0.7875498021711725"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e3deb",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction (PCA) + Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51b2907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: Ridge logistic regression\n",
      "accuracy of each fold - [0.67475, 0.695, 0.68475, 0.68, 0.685]\n",
      "Avg accuracy : 0.6839000000000001\n",
      "f1 of each fold - [0.6882338844955668, 0.7174617878647522, 0.6873295313662288, 0.6842624568327578, 0.6987087517934002]\n",
      "Avg f1 : 0.6951992824705412\n",
      "Method: LASSO logistic regression\n",
      "accuracy of each fold - [0.6755, 0.69525, 0.6835, 0.6805, 0.684]\n",
      "Avg accuracy : 0.6837500000000001\n",
      "f1 of each fold - [0.6891762452107278, 0.717497103128621, 0.6872529644268774, 0.685066535239034, 0.6971729755630092]\n",
      "Avg f1 : 0.6952331647136539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X = pd.read_csv('X_pca0.7.csv',sep=',',skiprows=1, header=None).to_numpy()\n",
    "testModel(LogisticRegression(solver = \"liblinear\", penalty = \"l2\"),\"Ridge logistic regression\")\n",
    "testModel(LogisticRegression(solver = \"liblinear\", penalty = \"l1\"),\"LASSO logistic regression\")\n",
    "testModel(MLPClassifier(solver='sgd', learning_rate = \"adaptive\", learning_rate_init = 0.1, alpha=1e-5,hidden_layer_sizes=(10, 2), random_state=1,max_iter=500),\"multilayer perceptron\")\n",
    "testModel(LinearSVC(penalty = \"l1\", loss=\"squared_hinge\",dual=False, max_iter = 10000, tol = 0.001),\"SVM\")\n",
    "testModel(DecisionTreeClassifier(random_state=0),\"decision tree\")\n",
    "testModel(RandomForestClassifier(max_depth=300, random_state=0),\"random forest\")\n",
    "testModel(GaussianNB(),\"Gaussian Naive Bayes\")\n",
    "testModel(KNeighborsClassifier(n_neighbors=5, metric=\"cosine\"),\"kNN (cosine)\")\n",
    "testModel(KNeighborsClassifier(n_neighbors=5, metric=\"euclidean\"),\"kNN (Euclidean)\")\n",
    "\n",
    "ridge = LogisticRegression(solver = \"liblinear\",penalty = \"l2\")\n",
    "testModel(AdaBoostClassifier(estimator = ridge, n_estimators=30),\"ridge + AdaBoost\")\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "testModel(AdaBoostClassifier(estimator = rf, n_estimators=10),\"random forest + AdaBoost\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
